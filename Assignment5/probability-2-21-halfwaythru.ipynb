{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Sci Engineering Methods and Tools, Week 6 Lecture 1</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 18 October 2021, with material from Cam Davidson-Pilon</div>\n",
    "\n",
    "* * * \n",
    "*At the end of this lecture, you should a good understanding of probability distributions, how to estimate probabilities for different outcomes and sample spaces, how to use Bayes' theorem to answer typical interview questions involving probabilities, and start thinking about finding a job in high-stakes sports, such as Formula 1 racing.*\n",
    "\n",
    "*For next week, reading homework: Chapter 6 of your textbook, Mathematics for Machine Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework solution\n",
    "\n",
    " Determine the probability that the sum of a three-dice roll is prime:\n",
    " \n",
    "<center>\n",
    "<img src=\"ipynb.images/3-dice-roll.png\" width=\"200\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "def p(event, space): \n",
    "    \"\"\"The probability of an event, given a sample space of equiprobable outcomes.\n",
    "    event can be either a set of outcomes, or a predicate (true for outcomes in the event).\"\"\"\n",
    "    if is_predicate(event):\n",
    "        event = such_that(event, space)\n",
    "    return Fraction(len(event & space), len(space))\n",
    "\n",
    "is_predicate = callable\n",
    "\n",
    "def such_that(predicate, collection): \n",
    "    \"\"\"The subset of elements in the collection for which the predicate is true.\"\"\"\n",
    "    return {e for e in collection if predicate(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = {1,2,3,4,5,6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D3 = {(d1, d2, d3) for d1 in D for d2 in D for d3 in D}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prime_sum(outcome): return is_prime(sum(outcome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dua Lipa prime code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def is_prime(n): return True if n in [x for x in range(3,36) if all(x%y!=0 for y in range(2,n//2))] else False\n",
    "def is_prime(n): return n > 1 and not any(n%i == 0 for i in range(2,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fraction(73, 216)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(prime_sum, D3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Introduction to Probability Distributions\n",
    "\n",
    "Your textbook, in page 6, states that the foundations of Machine Learning are:\n",
    "\n",
    "<center>\n",
    "<img src=\"ipynb.images/foundations-of-ML.png\" width=\"500\" />\n",
    "</center>\n",
    "\n",
    "And so let's start with a lab that introduces a very important probability distribution: The **binomial**.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"http://statistics.wdfiles.com/local--files/ch6/binomial.png\" width=\"400\" />\n",
    "Binomial\n",
    "</center>\n",
    "\n",
    ">**Definition**: The binomial distribution with parameters $n$ and $p$ is the discrete probability distribution of the number of successes in a sequence of $n$ *independent* experiments, each asking a yes–no question, and each with its own boolean-valued outcome: success/yes/true/one (with probability $p$) or failure/no/false/zero (with probability $q = 1 − p$).\n",
    "\n",
    "> If the random variable X follows the binomial distribution with parameters n ∈ ℕ and p ∈ [0,1], we write X ~ B(n, p). The probability of getting exactly k successes in n independent Bernoulli trials is given by the **probability mass function**: $$f(k) = {n \\choose k} p^{k} (1-p)^{n-k}$$\n",
    "\n",
    ">where $\\binom {n}{k}$ is our famous ***choose*** function from previous lecture: $${n \\choose k} = \\frac{n!}{k!(n-k)!}$$\n",
    "\n",
    ">The formula can be understood as follows: $k$ successes occur with probability $p^k$ and $n − k$ failures occur with probability $(1 − p)^{n − k}$. However, the $k$ successes can occur *anywhere* among the $n$ trials, and there are $\\binom {n}{k}$ different ways of distributing $k$ successes in a sequence of $n$ trials, just like there are $\\binom {23}{6}$ different ways of selecting 6 balls from an urn of 23.\n",
    "\n",
    ">**Example**: Suppose a **biased** coin comes up heads with probability 0.3 when tossed. The probability of seeing exactly 4 heads in 6 tosses is:\n",
    "\n",
    ">$$f(4, 6, 0.3) = {6 \\choose 4} 0.3^{4} (1 - 0.3)^{6 - 4} = 0.06$$\n",
    "\n",
    "A single success/failure experiment is also called a **Bernoulli** trial or Bernoulli experiment and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., $n = 1$, the binomial distribution is a Bernoulli distribution.\n",
    "\n",
    "*Why* is all this so important?\n",
    "\n",
    "So far, we have made the assumption that every outcome in a sample space (such as our urn) is ***equally likely***: Same probability to select White ball \\#1 as Red ball \\#5. Right? But what if we take a different perspective and compare the probability of selecting one red ball versus that of selecting one white one?\n",
    "\n",
    ">**Recall**: A probability is a **fraction** where the numerator is sum of all *favorable* outcomes, and the denominator the sum of all *possible* outcomes.\n",
    "\n",
    "In real life, we often get outcomes that are *not* **equiprobable**. For example, White ball versus Red ball in our previous experiment. For example, the probability that your professor marries Dua Lipa is not as probable as the probability that some famous rock start marries Dua Lipa, right? And we can't really count how many chances Dino has of marrying Dua Lipa versus how many chances the other rock star has by putting \"*Dino balls*\" and \"*rock star balls*\" in an urn, because sometimes we cannot decompose outcomes that quantitatively.\n",
    "\n",
    "For another example, the probability of a child being a girl is not exactly 1/2, and the probability is slightly different for a second child. \n",
    "\n",
    "An [article](http://people.kzoo.edu/barth/math105/moreboys.pdf) (good Data Science article, please read it) gives the following *counts* for two-child families in Denmark, where `GB` means a family where the first child is a girl and the second a boy:\n",
    "\n",
    "    GG: 121801    GB: 126840\n",
    "    BG: 127123    BB: 135138\n",
    "    \n",
    "Because of this fact (not all urns yield Blue/Red/White *numbered* balls with equiprobable chances), we need to introduce three more definitions:\n",
    "\n",
    "* [Frequency](https://en.wikipedia.org/wiki/Frequency_%28statistics%29): a number describing how often an outcome occurs. Can be a count like 121801, or a ratio like 0.515.\n",
    "\n",
    "* [Distribution](http://mathworld.wolfram.com/StatisticalDistribution.html): A mapping from outcome to frequency for each possible outcome in a sample space. \n",
    "\n",
    "* [Probability Distribution](https://en.wikipedia.org/wiki/Probability_distribution): The distribution above, which has been *normalized* so that the sum of the frequencies is 1.\n",
    "\n",
    "Here is an example of a ***super-popular*** probability distribution:\n",
    "* The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N.\n",
    "\n",
    "Oh noooooo...\n",
    "<center>\n",
    "    <img src=\"ipynb.images/miserable.png\" width=300 />\n",
    "</center>\n",
    "\n",
    "We need to again modify our awesome probability counting `p()` function (which takes each event as **equiprobable**) to take this miserable fact into account. Essentially, it's as if each *numbered ball* comes with its *own* probability of being picked. So we will need to use probability **dictionaries**.\n",
    "\n",
    "We love python dictionaries, right, so it should be fun, right?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/pin.gif\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's plot a few binomial distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAADrCAYAAAAcyZ8OAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAArVElEQVR4nO3deZxU5Z3v8c+vqveFpZtm64VmlaCypVkUFVyQBkxITGI0k3WSoJNgMpp4b/KaJHfuTDKTO9fxJnfUICaaRKMYvaggCAqKiUFkUdlFGhrotoGGZuu9u6qe+8epgqKp7j61dW2/t69+ddepc049h5Yvz3POs4gxBqWUSmWOWBdAKaViTYNQKZXyNAiVUilPg1AplfI0CJVSKU+DUCmV8tJiXYBABg0aZMrLy2NdDKVUktm+ffspY0xR1+1xGYTl5eVs27Yt1sVQSiUZETkSaLs2jZVSKU+DUCmV8jQIlVIpT4NQKZXyNAiVUilPg1AplfJSJghdbg9PbT7CuZbOWBdFKRVnUiYIn99ey09f2s26vcdjXRSlVJxJiSBs63Tzq/UfAXCqqT3GpVFKxZuUCMI/bDrMifPtiMDppo5YF0cpFWeSPgjPtXby6MaDzB5XRPGAbBqaNQiVUpdK+iB8/C+HONfayQPzrqAwN0ODUCl1maQOwvrGNn73djWfmjScq4r7U5iXyelmvUeolLpUUgfhw29U0en28IO54wAoyM2gQe8RKqW6SNogPNrQwjPvHuWL00opH5QLQGGe1TTWJUyVUv6SNggfen0/aU7hezePvbCtMDeDDpeHpnZXDEumlIo3SRmEVfWNvLyjjm/MGsmQflkXthfkZgJwWh+YKKX8JGUQ7qk7jzFw+5TiS7YX5mUAcErvEyql/CRlEPqavv2z0y/ZXphrBaHWCJVS/pIzCNusIMzNvHRJlsI8q2ncoMPslFJ+kjMI212IQE6G85LtvhqhdqpWSvlL2iDMy0xDRC7ZnpXuJDfDqX0JlVKXSM4gbLOCMJCCvAwdXaKUukRyBmF790FYmJupTWOl1CWSNwizugtCHWanlLpU8gZhdzXCvAztPqOUuoStIBSRShHZLyJVIvKjAO//nYjs9H5tEpFJfu8dFpFdIvKBiGyLZOG70+M9wtxMGprbdbyxUuqCwGnhR0ScwCPAXKAW2CoiK40xe/12qwZmG2POiMh8YBkww+/9G40xpyJY7h71VCMclJdBp9vQ2O6iX1Z6wH2UUqnFTo1wOlBljDlkjOkAlgOL/HcwxmwyxpzxvtwMlES2mMHp6R5hga8vod4nVEp52QnCYqDG73Wtd1t3vgm86vfaAK+JyHYRWRx8EYNjjOmxRlhwYZiddqFRSll6bRoDEmBbwBtsInIjVhBe57d5ljGmTkQGA6+LyIfGmL8EOHYxsBigrKzMRrECa+lwYww9NI2tYXY68YJSysdOjbAWKPV7XQLUdd1JRCYCvwUWGWMafNuNMXXe7/XAi1hN7csYY5YZYyqMMRVFRUX2r6AL34QLvTWN9cmxUsrHThBuBcaKyEgRyQDuBFb67yAiZcAK4CvGmI/8tueKSL7vZ+BWYHekCh/IhSDstWmsQaiUsvTaNDbGuERkCbAOcAJPGGP2iMg93veXAj8DCoFHveN7XcaYCmAI8KJ3WxrwjDFmbVSuxMs380x3QZiV7iQvM00XeldKXWDnHiHGmDXAmi7blvr9/C3gWwGOOwRM6ro9mnqrEYJ2qlZKXSrpRpb0do8QdDU7pdSlki8IvU3j/MzuO0vrxAtKKX/JF4Ttvtmpnd3uY028oPcIlVKWpA3CnprGvnuEOt5YKQVJGoQZTgeZad3XCAtyM3B5DOdbdX1jpVQyBmGbq8dmMVwcXdKgw+yUUiRjEPYw4YJPgS7ipJTyk5xB2MMTY9AZaJRSl0q+IGxzkd9DZ2q42DTWTtVKKUjGIGzv/R7hwFyrxqhdaJRSkKRBmNfLzNOZaU7ys9L0HqFSCkjWIOylaQzeTtUahEopkjEI21zk9dI0BijMy9RZqpVSQJIFocvtobXT3etTY9CJF5RSFyVVEDZ3uIGeh9f5DMrTprFSypJUQegbZ9xb9xmwaoSnmzvweHS8sVKpLrmCsM0384ydhyWZuD2G822d0S6WUirOJVcQtluhZqdpXJinw+yUUpYkC0LvPUKbNULQYXZKqWQLQt/s1DZqhLrQu1LKJ7mC0Ns0tnWP0Ns01oXelVJJFYSNvSzl6W9gjq5vrJSyJFUQNgdxjzAjzUG/rDSdeEEplVxB2NTeSXa6E6dDbO0/KE9Xs1NK2QxCEakUkf0iUiUiPwrw/t+JyE7v1yYRmWT32EiyMzu1Px1mp5QCG0EoIk7gEWA+MAG4S0QmdNmtGphtjJkI/CuwLIhjI6ap3W1rVImPbzU7pVRqs1MjnA5UGWMOGWM6gOXAIv8djDGbjDFnvC83AyV2j42kprbOIGuE2jRWStkLwmKgxu91rXdbd74JvBrisWFpaneRm2E/CAflZXCmRccbK5Xq7ARhoCcPAZNDRG7ECsL/HsKxi0Vkm4hsO3nypI1iXa6xLfh7hG6P4VyrjjdWKpXZCcJaoNTvdQlQ13UnEZkI/BZYZIxpCOZYAGPMMmNMhTGmoqioyE7ZL9Pc0fvCTf4KdX1jpRT2gnArMFZERopIBnAnsNJ/BxEpA1YAXzHGfBTMsZFkLe4eRBDqsp5KKaDX1DDGuERkCbAOcAJPGGP2iMg93veXAj8DCoFHRQTA5a3dBTw2StcSUvcZ0BlolEp1tlLDGLMGWNNl21K/n78FfMvusdHQ7nLT6Ta2RpX46FRcSilIopElwcw84+Mbb6zD7JRKbckThN5p+oPpPpPudNA/O50zWiNUKqUlTRBemHkmiBoh6PrGSqkkCsLmIBZu8jcw1+pUrZRKXUkThL6mcbA1woE5GZxu1g7VSqWypAvCYPoRgtU01un6lUptSReEITWNmzsxRscbK5WqkicIQ3xYUpCbTofbQ3OHOxrFUkolgOQJwnYXDoHsdGdQxxV4l/U8rcPslEpZSROEjd5xxt4hfrYV5KYDcFqfHCuVspImCJvbg5t5xsc3ukQ7VSuVupImCIOdcMGnMNc3FZcGoVKpKqmCMNiuMwADvU1jrREqlbqSKgiDmXnGJy8zjXSn6D1CpVJY8gRhmyuomWd8RISC3Ax9aqxUCkueIAxy4SZ/A3MytEaoVApLniAMcuEmfwW5GXqPUKkUlhRBaIyhKciFm/wV5OpC70qlsqQIwpYON8YEP7zOpyBXm8ZKpbKkCMJQZ57xGZiTwbnWTlxuTySLpZRKEEkRhBdmpw6jaWwMnNWF3pVKSUkRhBdmpw6jaQzaqVqpVJUUQXhhdurM9JCO9wWhPjBRKjXZqkKJSCXwa6xF2n9rjPlll/fHA08CU4F/MsY86PfeYaARcONd+D0yRb/I1zTOzQxuCi4f38QLCRWE1X+FF/4ePF2a82lZ8KU/w7CJsSmXUgmo1yAUESfwCDAXqAW2ishKY8xev91OA98DPtPNaW40xpwKs6zdurhwU2g1Qt9C7wn15Hjr41YIXv2FS7e/9xRs/z3c9lBMiqVUIrJTI5wOVBljDgGIyHJgEXAhCI0x9UC9iCyMSil7EerCTT4DchJs4oXWs7B/LVT8Pcz/5aXvtZyGPSug8peQlhGT4imVaOzcIywGavxe13q32WWA10Rku4gsDqZwdl3sPhNa0zgzzUleZlrirGa392Vwt8PEOy5/b+IXofUMVK3v+3IplaDsBGGgKZ+DWeloljFmKjAf+K6I3BDwQ0QWi8g2Edl28uTJIE5v3SPMcDrITAstCME3uiRBVrPb+WcoHAvDp1z+3ugbIWcQ7Hyu78ulVIKyE4S1QKnf6xKgzu4HGGPqvN/rgRexmtqB9ltmjKkwxlQUFRXZPT1g3SMMtVnsMzA3g9MtCVAjPFsDR962an6BliVwpsNVn4P9r1pNaKVUr+wE4VZgrIiMFJEM4E5gpZ2Ti0iuiOT7fgZuBXaHWtjuhDoXob+CnPTEuEe463nr+9Wf736fiV+0ms77bP2alEp5vQahMcYFLAHWAfuAPxtj9ojIPSJyD4CIDBWRWuB+4CciUisi/YAhwNsisgPYAqw2xqyN9EX4Fm4KR0FuZvx3nzHGavKWzoSCkd3vVzwVCkZbTWilVK9spYcxZg2wpsu2pX4/H8dqMnd1HpgUTgHtCHXhJn8FuenxH4THd8HJD2FhL11jRGDSnfDmL6ym9IDSnvdXKsUlzciSSNwjbO100xrPC73vfA4c6XDlZ3vf19e/cPcL0S2TUkkgaYIw3KZxYW6cd6r2uK37g2NvhZyC3vcvGAmlM2DHc1aTWinVraQIwsa28B+WxP36xtVvQdOJwH0HuzPxDji5D05E/PmUUkklKYKwuT20hZv8xf3ECzv/DJn9YVyl/WOuvB0cadqnUKleJHwQutweWjvd4dcI4zkIO5ph3yq4chGkZ9k/LqfAakrvesFqWiulAkr4IGxut/6CR+weYTwG4f5XoaPJ6h8YrIl3QOMxqP5L5MulVJJI+CBs6vDNPBNeEPbLSschcCYeH5Z8tA5yB0PZtcEfO24+pGXDgdciXy6lkkTiB2FbeDPP+DgcwsCcDBrisUZYuwXKZoAjhF9XepY1JrlmS+TLpVSSSPwgbLfGB4fbNIY4Xd+48QScOWx1hQlV6XQ4tgM62yJWLKWSScIHYbgLN/kbGI/rG9d6a3LhBqGnE459EJEiKZVsEj4IfQ9Lwu0+A1CQE4dBWLMFnBkwLIyRiiXeCX9q3o1MmWxq7GjkP7b+B68feb1PP1epYCV8EPqaxpGoERbkZcTfw5KaLTBsMqRlhn6OvCIoGNWn9wn3Nuzli698kaf2PsX9G+/n55t/Trs7QeZ7VCkn4YPw4sJNkakRnmnpxOOJkyFprg6oe99q2oarZLoVhFEebmeMYfmHy/nymi/T4e7giXlP8I0rv8Fz+5/jy2u+zNHzR6P6+UqFIuGD8OJSnpG5R+j2GM63xckErcd3WvMKRiIIS6dDc7314CVKGjsa+cFbP+AX7/6CmcNm8sKnXmDa0GncX3E/D9/0MMeaj3HHK3ewtjriM7EpFZaED8Lbp5Tw5Nen4XQEWlEgOHHXqdp3T68kQkEIULs1/HMFcK79HHe+cidvHH2D+z95Pw/f/DADsgZceH926Wyev+15xgwYwwN/eYDHdjwWlXIoFYqED8KywhxuHD84IufyDbOLm/uENVugfxn0Gxb+uQZPgIy8qD0weXrf0xxtPMpjcx/jG1d9A4dc/r/WsLxhPFn5JHNHzGXZzmWcbAlubRqloiXhgzCSCrwz0DQ0xVEQRqJZDOBwQvEno/LApLGjkT/t/RM3l93MjGE9d/NJd6Rz39T7cBs3T+55MuJlUSoUGoR+CvLiqEZ4rhYa6yIXhGD1RTyxG9qbIndO4Jl9z9DY2cjiifZWay3tV8qCkQt4fv/zNLQ2RLQsSoVCg9CPr0YYF+sb+5qwEQ3C6WA88PH2iJ2yubOZp/Y9xeyS2UwonGD7uG9P/Dbt7nb+sPcPESuLUqHSIPSTneEkK90RH+sb12yB9BwYclXkzllSYX2vjVzz+Ln9z3Gu/Rx3T7w7qONG9h9JZXklyz9cztm2sxErj1Kh0CDsojA3M05qhFtg+FRrneJIyR4IReMjdp+w1dXKH/b8gWuHX8vVRVcHffy3J36bVlcrT+17KiLlUSpUGoRdDMxNj/09ws5Wqw9hJJvFPiXTrC40Hk/Yp3p+//OcbjsddG3QZ+zAsdxSdgvP7HuG8x3nwy6PUqHSIOwiLqbiqnsfPK7oBGHpDGg9Aw1VYZ2m3d3O7/f8nulDpzN1yNSQz3P3pLtp6mzimX3PhFUepcKhQdhFXEzFFcmO1F1d6FgdXvN4xYEVnGw9GXJt0Gd8wXjmlMzhqb1P0dQR2afZStllKwhFpFJE9otIlYj8KMD740XkHRFpF5EfBnNsvImPINwChWMgtzDy5y4cC1kDwupY3eHu4He7fseUwVOYNnRa2EW6e9LdnO84z/L9y8M+l1Kh6DUIRcQJPALMByYAd4lI134Sp4HvAQ+GcGxcKcjJoLHdRbsrRosdGWMFYTRqg2DNcl0yDWpCH2q37vA6TrScYPHExYiEP7TxqkFXMWv4LJ7e+zQujyvs8ykVLDs1wulAlTHmkDGmA1gOLPLfwRhTb4zZCnR93NrrsfHGN8zubEuMnhyfPgQtp6Jzf9CndIa13nHr2ZAOX31oNcNzh3Pt8BDWUOnG58d9noa2Bt491rdzJioF9oKwGKjxe13r3WZHOMfGRMwnXvBNihDVIPQ2Z2u3BX3oqdZTvHPsHRaMWhBwPHGori+5nvz0fFYfWh2xcypll53/kwO1fexOamf7WBFZLCLbRGTbyZOxG4wf8/WNa96FzH5Wf79oKf4kiCOkBybrDq/DYzwsHLkwokXKdGYyt3wuG45uoNXVGtFzK9UbO0FYC5T6vS4B6mye3/axxphlxpgKY0xFUVGRzdNHXkHMg3CrFVQOZ/Q+IzMfBl8Z0gOT1YdWc8XAKxgzcEzEi7Vw5EJaXC1srNkY8XMr1RM7QbgVGCsiI0UkA7gTWGnz/OEcGxMFsZyKq70J6vdYDzOiraQCPn4vqI7VR84fYdepXSwcFdnaoE/F0AoG5wzW5rHqc70GoTHGBSwB1gH7gD8bY/aIyD0icg+AiAwVkVrgfuAnIlIrIv26OzZaFxMJA7KtIW0xmYrr2AfWpAh9EoTToP08NBywfcia6jUIwvyR86NSJIc4WDByAX/7+G+caTsTlc9QKhBbd7uNMWuMMeOMMaONMb/wbltqjFnq/fm4MabEGNPPGDPA+/P57o6NZ2lOB/2zYzTMzvegxDc5QjT5wtbmjNXGGNYcWkPF0AqG5g6NWrEWjlqIy7h05TvVp3RkSQCFsVrfuHYbFIyGnILof1bhGMjqbzsI9zbs5fD5wxF/SNLVFQOvYHT/0do8Vn1KgzCAQfmZnDjf1rcfaowVSn3RLAarY3Vxhe0uNK8ceoV0Rzpzy+dGtVgiwsJRC3mv/j3qmuw+k1MqPBqEAZQV5HCkoaVvP/RcDTSd6JtmsU9JBdTv7XXGarfHzdrDa7mh5Ab6ZfSLerEWjFoAWPckleoLGoQBlBfmUN/YTmtHHw6z89XM+jQIp1kPZ+re73G3d4+/y6nWU1F7WtxVcV4xUwZPYfWh1Zgor8OsFGgQBlRWmAvA0dN9WCus3QZpWZGdkbo3xZ/0fnbP9wlXH1pNfno+N5Tc0AeFsiwcuZCqs1V8dOajPvtMlbo0CAMYUZADwOGG5r770NqtMHxKZGek7k1OgfXQpIf7hG2uNjYc3cAtI24h05nZZ0W7tfxW0iRNH5qoPqFBGMCIQisIj/bVfUJXOxzb0bfNYh/fjNXdNEE31m6kubO5z5rFPgOzBjKreBarq1fjMeHPpq1UTzQIAxiQk0H/7HSOnO6jGuHx3eBut57i9rXiT0JzPZw9GvDttdVrKcouomJI35dtwcgF1LfU8359z/cwlQqXBmE3RhT24ZPjCx2p+6jrjL8eOlY3dTTx19q/cmv5rTijOfa5G3NK55DlzGJt9do+/2yVWjQIu9GnXWg+3gb5w6F/DGYoG3IlpGUHXOv4zZo36fB0UFle2fflAnLSc7i+5HpeP/I6bk+MJspVKUGDsBsjCnP4+Gwrne4+uD9VuzU29wfBejgzfErAGuFrh19jSM4QJhZNjEHBLPPK59HQ1sD2E5FblF6prjQIuzGiMBe3x1B3Nspz4zWdhDOHY9Ms9impsB7WuC4ubH++4zxv173NvPJ5EZ2ANVg3lNxAdlo2aw9r81hFjwZhN3xdaKLePP7Y15E6lkE4DdwdcHzXhU1vHn0Tl8fFvPJ5sSsXkJ2WzeyS2aw/sl7XM1FRo0HYjRHeTtVHot2XsHYriBOGTYru5/TE1yz3ax6vPbyW4rxirh50dYwKdVFleSVn2s+w5Xh4S5Aq1R0Nwm4Mzs8kM80R/Rph7TYYehVk5ET3c3rSbzj0K77Qsfps21k2123m1vJbI7JKXbiuK7mOnLQc1h1eF+uiqCSlQdgNh0OsLjTRHGbncVuzRMeyWexTUnGhRvhGzRu4TOybxT6ZzkxuLLuR9UfW0+mJ0eqCKqlpEPagrCA3uqNLTu6HjsY4CcJpcPYINNWztnotpfmlTCiInyWoK8srOd9xns11m2NdFJWENAh7YNUIm6M3A0osO1J35S3D6eqNbDm+hcryyrhoFvtcO/xa8tPz9emxigoNwh6MKMyhrdNDfWN77zuHonYrZA+EglHROX8whk0CRxrrD63Gbdxx0yz2yXBmcGPZjbx59E063DFaYVAlLQ3CHlx8chyl5nHtNmt8cTzUvNKzYchVrDuzh/J+5YwbOC7WJbpMZXkljZ2NbKrbFOuiqCSjQdiDi30Jo9CFpvEEnNwHI66J/LlDdKpkKttoY17pTXHVLPaZOWwm/TL66dNjFXEahD0oHpiN0yHRqREefMP6PvrmyJ87RK/n98cjQmX6oFgXJaB0Zzq3jLiFN2vepN0dpdsVKiVpEPYg3elg+ICs6HShObgBcgbB0NiN4+1qdeNHjOlwMebY3lgXpVuV5ZU0dzbzZs2bsS6KSiK2glBEKkVkv4hUiciPArwvIvJ/ve/vFJGpfu8dFpFdIvKBiNhbMi2OlBfmcjTSTWOPx6oRjr7JWk0uDhw8e5Adp3bxmcwhULUh1sXp1vSh0xmaO5SXDrwU66KoJNLr30IRcQKPAPOBCcBdItK1g9l8YKz3azHwmy7v32iMmWyMidEUK6ErK4hCp+rjO6ClAcbcEtnzhmHFgRWkSRq3lS+EU/vhXG2sixSQ0+HkM2M+w6a6Tbrcp4oYO9WR6UCVMeaQMaYDWA4s6rLPIuCPxrIZGCAiwyJc1pgYUZjD2ZZOzrVEcESDr8Y1+qbInTMMne5OVh1cxZzSORSOv83aGMe1ws+M+QwAL1e9HNuCqKRhJwiLgRq/17XebXb3McBrIrJdRBaHWtBYKSvwdqGJ5LT9B9+w7g3mFUXunGHYWLuRM+1n+OzYz0LReGuS2Kr1sS5Wt4rzipkxbAYvVb2k65moiLAThIH6UXQdatHTPrOMMVOxms/fFZGAa0KKyGIR2SYi206ePGmjWH2jfFCEp+NqOw8178KY+HlavOLACgbnDGbW8FlWn8YxN8Ght8Adv9Ne3T72duqa69h8TIfcqfDZCcJaoNTvdQnQ9eZMt/sYY3zf64EXsZralzHGLDPGVBhjKoqK4qOmBNY9QojgGsfVfwGPK266zRxvPs6muk0sGr3o4rokY26B9nMBp++PFzeV3UT/zP68eODFWBdFJQE7QbgVGCsiI0UkA7gTWNlln5XAV71Pj2cC54wxx0QkV0TyAUQkF7gV2B3B8kddTkYaRfmZketUfXADZORB6YzInC9ML1e9jMd4rGaxz6g5IA6rrHEq05nJbaNuY8PRDZxtOxvr4qgE12sQGmNcwBJgHbAP+LMxZo+I3CMi93h3WwMcAqqAx4HveLcPAd4WkR3AFmC1MSbhRs2PKMjhcCSaxsZYDyHKr4e0jPDPFyaP8fBi1YtMHzqd0ny/Cn32QGuZzzh+YALw2TGfpdPTyepqXQRehSfNzk7GmDVYYee/banfzwb4boDjDgExnHo5MsoKc9hU1RD+iU4fsqa6uvbe8M8VAVuPb+Xjpo9ZMmXJ5W+Ovhne+l/QchpyCvq+cDZcUXAFVxZeyYoDK/jS+C/F5bBAlRjiozdvnCsvzOX4+TbaOsNcUtJXw4qTByUrDqwgPz2fW8oC9GccczNg4FB8j+C4feztfHTmI/Y2xO9oGBX/NAhtGFFoPTCpCfeBycENMHBkXEy7da79HOuPrGfBqAVkpWVdvsPwqZA1AKre6POyBWP+yPlkObNYcWBFrIuiEpgGoQ2+J8dh3Sd0dUD1X+OmNrimeg0dng5uH3t74B2cadZDk4MbrHubcSo/I5+5I+aypnoNra4oL72qkpYGoQ0RWdGuZjN0NsdFtxmP8fD8R88zvmA8Ewp7mI5/zM3QeAzq9/Vd4ULw2bGfpamziVerX411UVSC0iC0YWBOOvlZaeH1JaxaD440GHl95AoWovVH1nPgzAG+OuGrPe/oC+047kYDUDGkggmFE1i2c5ku7qRCokFog4h3RbtwmsZVb0DpTMjMj1zBQuD2uHn0g0cZ1X8UC0Yu6Hnn/sXWkLs4Hm4H1u9nyeQlfNz0MS9VvRTr4qgEpEFo04jCXA6ebArt4NPVcGJXXNwffPXwqxw8d5DvTP7OxZEkPRlzCxzZBM0R6D4URdcVX8ekokk8tuMxnbRVBU2D0KaZIwuoPdPKh8fPB3/wu49ZzeJJd0a+YEHo9HTymw9+wxUDr2DuiLn2Dpr8d+DugO1PRLdwYRIR7p1yLydaTvDCRy/EujgqwWgQ2jT/6mE4HcKqHUHOgdd2Dt5/Cq76HPQbHp3C2bTq4CqONh5lyZQlOMTmr37IBGu6sC2Pgyu+a1ozhs1g+tDpPL7zcX2CrIKiQWjToLxMrh1dyKodx4Jb5/i9P0JHE8z8Tu/7RlGHu4OlO5Zy9aCrmV0yO7iDr/kuNJ2A3fHfV2/JlCU0tDWw/MPlsS6KSiAahEH49KThHD3dwo7ac/YOcLtg81JrbPHwyVEtW29WHFjBseZjLJm8JPihaKNvth6avPNIXPcpBJgyeAqzimfxxO4naOoI8Z6uSjkahEG49cqhZDgdrPzAZvN438twvtaqUcVQm6uNZTuXMXXwVK4ZHsLyoSLWNZzYZU0jFufunXwvZ9vP8vS+p2NdFJUgNAiD0D87nTlXFPHKzjrcnl5qRsbApoehYDSMndc3BezGc/uf42TrSe6dcm/oExNcfYe16t47j0S2cFFw5aAruan0Jv6454+ca7dZe1cpTYMwSJ+aNJz6xna2VJ/uecead6HuPbjmOzFdqe5U6yl+t+t3zBw2k4qhYaydlZ4F078NB9bBqQORK2CUfGfyd2jsbOQ3O7quI6bU5TQIg3TzJwaTk+Fk1c5emsfvPGxNWjDprj4pVyCdnk5++NYPaXW18sC0B8I/YcU3wZkJmx8N/1xRdkXBFdw1/i7+tO9PrDu8LtbFUXFOgzBIORlp3PKJIby66xid7m4WDjp9CPa9AhV/Dxm5fVtAP7/a/iu2n9jOz675GeMGjgv/hHlFMPEO+ODZuO9gDfBAxQNMLJrIT//2Uw6dPRTr4qg4pkEYgk9NGs6Zlk7erjoVeAdfB+rpsVu0b231Wv6494/cNf4uPjX6U5E78TXfBVdr3HewBkh3pvPQ7IfITsvm+29+X58iq25pEIbghnGD6JeVFrhzdctpeM/XgTo2SztXnaniZ5t+xuSiyTxQEYEmsb/Bn7C602x5HDrjv9PykNwhPDj7QWoaa/jp334aXB9QlTI0CEOQmeak8qqhvLbnxKWzVrs64Pmvg7s9ZtPxN3Y0ct/G+8hJy+E/5/wn6c70yH/IdfdZHaxf+gfwxP+6wtOGTuO+T97H+qPreXLPk7EujopDGoQh+vSkYpraXWzcX29tMAZW3gvVb8Gn/wuGXtXnZXJ73Pzk7Z9Q01jDg7MfZHDO4Oh80Mjr4dafw54X4fWfRuczIuyrE77KvPJ5/Pq9X/NO3TuxLo6KMxqEIZo5qoBBeRms9DWP3/g57FwON/4TTP5Sn5fnZMtJ7l5/N2/UvMH9n7w/vK4ydlyzBKbfbT0d37y09/1jTET4l2v/hZH9RrJkwxKe/fBZbSarCzQIQ5TmdLDw6mGs31fP4XUPw18fhKlfhRsifE/Ohr99/Dc+v+rz7KjfwT9f8898ZcJXov+hIlD57zD+Nlj7I9jbdanr+JOTnsMTlU8wfdh0/u3df+O+jfdph2sFaBCG5e7Zo/lc3h5KNv2U+iHXw8L/YwVEH+n0dPLQ9oe4Z/09FGQVsPy25Xxu3Of6bllLhxM+91somQYrvg1H3+2bzw1DQVYBj9z8CD+s+CFv1b7FF1Z9gQ/qP4h1sVSMaRCGyu1i+NFX+Df3f3IkfRRzjnyDpW8f6ZPmlsd42Hp8K19f+3We3P0kXxj3BZ5d+CyjB4yO+mdfJj0b7loO/Yrh2S/Ch6vj/gGKQxx87cqv8dT8p3CKk6+v/ToPv/8wJ1tOxrpoKkbEzl9cEakEfg04gd8aY37Z5X3xvr8AaAG+box5z86xgVRUVJht27YFeSl9pKMF3n8a3vkvOHsUBl9J210v8MDaE6zaUceXZpTxL5++kjRn5P+NqW2sZdXBVbx88GU+bvqY/Ix8/sc1/4N55bEdywxYncifuh3OVMOgK2DW96zxyWkZsS5Zjxo7GvnXzf/Kq9Wv4hQn1w6/lkVjFjGndA6ZzsxYF09FmIhsN8ZcdgO91yAUESfwETAXqAW2AncZY/b67bMAuBcrCGcAvzbGzLBzbCBxF4SuDusv+J6XYMtj0NIApTNg1j/CuEpwOPB4DA++tp9HNx7k+rGD+MrMEUwsGcCQfpkhNVXdHje1TbVUna3i0NlDvHPsHbYe34ogzBg2g0VjFnFz2c1kp2VH/HJD5nbB3pfgb7+C47sgf5g1D+P4hTBghLVEaJyqPlfNqoOrWHlwJSdaTtAvox9zR8xlQuEERg8YzZgBY+if2T/WxVRhCicIrwH+2Rgzz/v6xwDGmH/32+cxYKMx5lnv6/3AHKC8t2MDCSYIjx//gPqG/Ze/Eei6jAG8X8bj/TLWVPSuNivwXG1WP8DmU3CuFs7VwPljgAcDUHoNTPwiDLvKe8pLP2f93uM8/nY1LrcH8NA/N51Rg3IoH5RNXpbgdBicTg8Ohxunw0OHp5VWVyMt7iZaXY00u5poaD/BsZYjdHo6Lpx3WE4Zs4bMY9bQWynMGmLrzyZmjCG/7q8M2bmU/GObrE2SRnu/Mtr6j6K9/2g6s4vwpGXjScvGOLOsn50Z3nusDow4QByAYC78O+L/D0p07oO6jYfdzR/x5tktbG/cTavn4qzcA9L6UZI5hH7OPHKd2eQ5c8hxWN8zHOmkiROnOEkTJ2mShgPBIQ7kwn94/1GUAFfTd/eWk8WQgWVMv/qWoI7pLgjt/BNdDNT4va7FqvX1tk+xzWPD8uLm/82j53ZG8pSXygPy/Prjuavh/V/C+90fklEKvgZhJ7Af2N/L0FzjzsB4sjHubIyrH572Gbjbh+BpH4KnYzCNnkw+Ap7kIHAwnCvqI2nAEsbJIq6WakY56hh1+hijznxIubxFpsTvspsTgDuw/sk87nRSlZHOwfR0DmY0UZ1+kjqHg/NOB+cdDlx9+HBMXWrmh/lBB2F37ARhoN901+pWd/vYOdY6gchiYDFAWVmZjWJZFk79B646/l4370qAH8WqdXhrGyDgzLDuZTkzIS0T0tIhLSfgE+BAzVy58C+8XLKfQxwX1gbpdBtcLgcejwO3x4HL5cTldpDuyCLbkYtIGgaDx1xey4Ru/tASwrRLXtUBdR43TlcLTncrDlcrTncbTlcr4ulAMGAM4q2xi+/KL/kz6ds/jX7AFO+XP2MM7aaTZk8bLuPChRuXsb7cuPEYg+8/D+aS36vB/+dAEvc33leGFUTu4aCdIKwFSv1el2D9/2xnnwwbxwJgjFkGLAOraWyjXACUlV1HWdl1dndXSqnL2Hm0uRUYKyIjRSQDuBPo2nt2JfBVscwEzhljjtk8VimlYqrXGqExxiUiS4B1WF1gnjDG7BGRe7zvLwXWYD0xrsLqPvONno6NypUopVSIbPUj7Gtx131GKZUUuntqrCNLlFIpT4NQKZXyNAiVUilPg1AplfLi8mGJiJwEjgRxyCCgm5WUEpJeT3xLputJpmuB3q9nhDGmqOvGuAzCYInItkBPghKVXk98S6brSaZrgdCvR5vGSqmUp0GolEp5yRKEy2JdgAjT64lvyXQ9yXQtEOL1JMU9QqWUCkey1AiVUipkCR+EIlIpIvtFpEpEfhTr8gRLRJ4QkXoR2e23rUBEXheRA97vA2NZRrtEpFRE3hSRfSKyR0S+792eqNeTJSJbRGSH93r+p3d7Ql4PWEtviMj7IvKK93XCXguAiBwWkV0i8oGIbPNuC/qaEjoIvWuiPALMx5pY+C4RmRDbUgXt90Bll20/AjYYY8YCG7yvE4EL+IEx5hPATOC73t9Hol5PO3CTMWYSMBmo9E4zl6jXA/B9YJ/f60S+Fp8bjTGT/brNBH9NxpiE/QKuAdb5vf4x8ONYlyuE6ygHdvu93g8M8/48DNgf6zKGeF0vYy3clfDXA+QA72EtNZGQ14M1MfIG4CbgFe+2hLwWv2s6DAzqsi3oa0roGiHdr5WS6IYYa2JbvN8H97J/3BGRcqzZ7d8lga/H25T8AKgHXjfGJPL1/Ar4b4D/wtOJei0+BnhNRLZ7l/uAEK4pftdXtMf2miiq74hIHvD/gH80xpwPZTnTeGGMcQOTRWQA8KKIXBXjIoVERG4D6o0x20VkToyLE0mzjDF1IjIYeF1EPgzlJIleI7SznkoiOiEiwwC83+tjXB7bRCQdKwT/ZIxZ4d2csNfjY4w5C2zEup+biNczC/i0iBwGlgM3icjTJOa1XGCMqfN+rwdeBKYTwjUlehAm65ooK4GveX/+Gta9trgnVtXvd8A+Y8xDfm8l6vUUeWuCiEg2cAvwIQl4PcaYHxtjSowx5Vh/T94wxnyZBLwWHxHJFZF838/ArcBuQrmmWN/sjMDN0gXAR1iL/f5TrMsTQvmfBY5hLYFcC3wTKMS6qX3A+70g1uW0eS3XYd2a2Al84P1akMDXMxFrBeud3r9gP/NuT8jr8buuOVx8WJKw1wKMAnZ4v/b4/v6Hck06skQplfISvWmslFJh0yBUSqU8DUKlVMrTIFRKpTwNQqVUytMgVEqlPA1CpVTK0yBUSqW8/w/MDEJEFIVUagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x270 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# n and p are exactly as defined above\n",
    "import numpy as np\n",
    "from scipy.stats import binom\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n_values = [20, 25, 40]\n",
    "p_values = [0.1, 0.5, 0.5]\n",
    "x = np.arange(0, 50)    \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3.75))\n",
    "\n",
    "for (n, p) in zip(n_values, p_values):\n",
    "    # create a binomial distribution\n",
    "    dist = binom(n, p)\n",
    "\n",
    "    plt.plot(x, dist.pmf(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximations\n",
    "\n",
    "We see that if $n$ is large enough and the skew $p$ of the distribution is not too great (not too close to 0 or too close to 1), then a reasonable approximation to B(n, p) is given by a **normal distribution**!. In fact, it can be proven that it is close to:\n",
    "\n",
    "$$N(np, np(1-p))$$\n",
    "\n",
    "if both values $np$ and $n(1-p)$ are greater than or equal to 5. This approximation, known as de Moivre–Laplace theorem, is a huge time-saver when undertaking calculations by hand (exact calculations with large n are very onerous). Historically, it was the first use of the normal distribution, introduced in Abraham de Moivre's book *The Doctrine of Chances* in 1738.\n",
    "\n",
    "The binomial distribution also converges towards another distribution that we have not seen yet: the **Poisson distribution**, as the number of trials goes to infinity while the product $np$ remains fixed or at least $p$ tends to zero. Therefore, the Poisson distribution with parameter $λ = np$ can be used as an approximation to B(n, p) if $n$ is sufficiently large and $p$ is sufficiently small. According to two rules of thumb, this approximation is good if $n ≥ 20$ and $p ≤ 0.05$, or if $n ≥ 100$ and $np ≤ 10$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit of Python-fu\n",
    "\n",
    "Now, let's digress a bit into abstract computer science..\n",
    "\n",
    "In Python, `*args` and `**kwargs` is a common idiom to allow ***arbitrary number of arguments*** to functions. `*args` will give you all function parameters as a tuple, `**kwargs` will give you all keyword arguments (except those corresponding to a formal parameter) as a dictionary:\n",
    "```python\n",
    "def foo(*args):\n",
    "    for a in args:\n",
    "    print a\n",
    "        \n",
    "def bar(**kwargs):\n",
    "    for a in kwargs:\n",
    "        print a, kwargs[a]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s say we want to create a higher-order function that takes as input some function $f$ and returns a new function that for any input returns *twice* the value of $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_doubler(f):\n",
    "    def my_g(x):\n",
    "        return 2 * f(x)\n",
    "    return my_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.my_doubler.<locals>.my_g(x)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_doubler(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works.. in most cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "def f_plus_1(x):\n",
    "    return x + 1;\n",
    "\n",
    "h = my_doubler(f_plus_1)\n",
    "print(h(3)) #(3+1) * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "my_g() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4e48114d1669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_doubler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_sum2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: my_g() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "def my_sum2(x, y):\n",
    "    return x + y;\n",
    "\n",
    "h = my_doubler(my_sum2)\n",
    "print(h(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh *nooooo*!\n",
    "\n",
    "What we need is a way to specify a function that takes *arbitrary arguments*. This is where Python's `*args` and `**kwargs` come into play:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unnamed args:  (1, 2, 3, 4)\n",
      "keyword args:  {'key1': 'NU', 'key2': 'rocks!', 'key3': 'really!', 'key4': 'ah-hah!'}\n"
     ]
    }
   ],
   "source": [
    "def magic(*args, **kwargs):\n",
    "    print (\"unnamed args: \", args)\n",
    "    print (\"keyword args: \", kwargs)\n",
    "magic(1, 2, 3, 4, key1 = 'NU', key2 = 'rocks!', key3 = 'really!', key4 = 'ah-hah!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/magic.png\" width=300 />**Ohhhhhhhhhhhh**</a><br>你真聪明\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "args is a `tuple` of its unnamed arguments and kwargs is a `dictionary` of its named arguments. So now we can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_new_doubler(f):\n",
    "    \"\"\"works no matter the inputs\"\"\"\n",
    "    def my_g(*args, **kwargs):\n",
    "        return 2 * f(*args, **kwargs)\n",
    "    return my_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "h = my_new_doubler(my_sum2)\n",
    "print(h(1, 2))  # 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, ***now*** are ready to define **Probability distributions** in python, our new ***p*** function, where each outcome on the x-axis has its own distinct probability of being picked: One (high) value for rock star, one (low) value for Dino for the outcome of marrying Dua Lipa.\n",
    "\n",
    ">**Guess**: What python data structure are we likely to use to define probability distributions?\n",
    "\n",
    "We define `ProbDist` to take the same kinds of arguments that a `dict` does: either a **mapping** (from item to its probability) or a **set** of (key, val) pairs, and/or optional keyword arguments (because each ball in the urn is *special* now: it has its *own* probability of being picked). \n",
    "\n",
    ">**A dose of reality**: It's like all boys/girls are not equal! You will not just pick any boy/girl to be your girl/boyfriend! There are some that have a *much higher chance* of being picked by you (related to *your* taste)!\n",
    "\n",
    "This is the first time (in class), that we will define a Python `class`, instead of a Python **function/lambda**. That is why we will define its **constructor** `__init__()`. We assume `self` (`this` in Python) is composed of a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class ProbDist(dict):\n",
    "    \"\"\"A Probability Distribution; an {outcome: probability} mapping.\"\"\"\n",
    "    def __init__(self, mapping=(), **kwargs):\n",
    "        self.update(mapping, **kwargs)\n",
    "        # Make probabilities sum to 1.0; assert no negative probabilities\n",
    "        total = sum(self.values())\n",
    "        for outcome in self:\n",
    "            self[outcome] = self[outcome] / total\n",
    "            assert self[outcome] >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We also need to modify the functions `p` and `such_that` to accept either a sample space as we had previously, or a probability distribution as the second argument `space`. \n",
    "\n",
    ">**Oh-oh**: Now we need to branch out on the ***2nd argument*** of function `p`!\n",
    "\n",
    "If we have a probability distribution, instead of *counting* each possible outcome equiprobably and thus just summing up `1`s (numerator: sum of all *favorable* outcomes, denominator: sum of all *possible* outcomes), we need to sum up the different discrete probabilities of each possible outcome: `sum(space[o] for o in space if o in event)`. \n",
    "\n",
    "We also need to modify `such_that()`, which is the set of all outcomes of our sample space for which the predicate (first) argument is `True`, so that its second argument can also be a `ProbDist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def p(event, space): \n",
    "    \"\"\"The probability of an event, given a sample space of equiprobable outcomes. \n",
    "    event: a collection of outcomes, or a predicate that is true of outcomes in the event. \n",
    "    space: a set of outcomes or a probability distribution of {outcome: frequency} pairs.\"\"\"\n",
    "    # branch on the type of the first argument\n",
    "    if is_predicate(event):\n",
    "        # transform the mapping (untangible) 'event' into the collection (tangible) 'event'\n",
    "        event = such_that(event, space)\n",
    "        \n",
    "    if isinstance(space, ProbDist):\n",
    "        # if space is a dictionary of distinct probabilities, where each item does not count as the same amount\n",
    "        # we need to be careful and count each amount according to what it's worth\n",
    "        return sum([space[o] for o in event])\n",
    "    else:\n",
    "        # space is not a dictionary but a collection, let's fall back to our original division\n",
    "        return Fraction(len(event & space), len(space))\n",
    "\n",
    "is_predicate = callable\n",
    "\n",
    "def such_that(predicate, space): \n",
    "    \"\"\"The outcomes in the sample pace for which the predicate is true.\n",
    "    If space is a set, return a subset {outcome,...} with outcomes where predicate(element) is true;\n",
    "    if space is a ProbDist, return a ProbDist {outcome: frequency,...} with outcomes where predicate(element) is true.\"\"\"\n",
    "    if isinstance(space, ProbDist):\n",
    "        return ProbDist({o:space[o] for o in space if predicate(o)})\n",
    "    else:\n",
    "        return {o for o in space if predicate(o)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "And now we can finally take on the Danes from the data science paper!\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/Danes.png\" width=300 />\n",
    "</center>\n",
    "\n",
    "# Danish two-child families: An introduction to *conditional* and *marginal* probabilities\n",
    "Here is the probability distribution for Danish two-child families as a `dictionar`y, describing the probability of each possible outcome, according to the data science paper referenced above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GG': 0.23840384261560926,\n",
       " 'GB': 0.24826679089140383,\n",
       " 'BG': 0.24882071317004043,\n",
       " 'BB': 0.2645086533229465}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DK = ProbDist(GG=121801, GB=126840,\n",
    "              BG=127123, BB=135138)\n",
    "DK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's write some useful predicates (in lambda syntax):\n",
    "```python\n",
    "def first_girl(outcome):  return outcome[0] == 'G'\n",
    "def first_boy(outcome):   return outcome[0] == 'B'\n",
    "def second_girl(outcome): return outcome[1] == 'G'\n",
    "def second_boy(outcome):  return outcome[1] == 'B'\n",
    "def two_girls(outcome):   return outcome    == 'GG'\n",
    "```\n",
    "Using these predicates, answer the following questions:\n",
    "\n",
    "* **Question 1**: What's the probability for a girl, and is it higher or lower for a ***second*** girl?\n",
    "* **Question 2**: Is the sex of the younger (second) child more likely or less likely to be the same as the first child? And is it more likely to be a boy or a girl?\n",
    "\n",
    "*Hint:* You will leverage the probability of a first girl, the probability of a second girl given that there was a first girl, and the probability of a second girl given that there was a first boy. In other words: `p(first_girl, DK)`, `p(second_girl, such_that(first_girl, DK))`, and `p(second_girl, such_that(first_boy, DK))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def first_girl(outcome):  return outcome[0] == 'G'\n",
    "def first_boy(outcome):   return outcome[0] == 'B'\n",
    "def second_girl(outcome): return outcome[1] == 'G'\n",
    "def second_boy(outcome):  return outcome[1] == 'B'\n",
    "def two_girls(outcome):   return outcome    == 'GG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4866706335070131"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(first_girl, DK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4872245557856497"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(second_girl, DK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Student Question**: Can we write the probability of 'BG' as p(second_girl and first_boy, DK)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, False, True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    (second_girl and first_boy)('GG'), (second_girl and first_boy)('BB'), \n",
    "    (second_girl and first_boy)('GB'), (second_girl and first_boy)('BG')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... Why is `(second_girl and first_boy)('BB') == True`?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(second_girl and first_boy)('BB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_girl('BB'), first_boy('BB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get `False` and `True`, and I know that 'False and True == False`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "False and True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is going on? To me, what it means is that `if f and g are two boolean functions, (f and g)(x) != f(x) and g(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function __main__.second_girl(outcome)>,\n",
       " <function __main__.first_boy(outcome)>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_girl, second_girl and first_boy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, instead of doing `p(second_girl and first_boy, DK)`, we hve to rewrite our `p` function so it takes in a ***collection*** of predicates as its first argument! We define two new functions, `p_or` and `p_and`, where the former considers the union of predicates, and the latter the intersection (we also restrict ourselves to the case where the first argument is of type predicate and the second of type probability distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_or(event_collection, space): \n",
    "    \"\"\"The probability of an event collection union, given a sample space of equiprobable outcomes. \n",
    "    event: a collection of predicates that are true of outcomes in the event.\n",
    "    space: a probability distribution of {outcome: frequency} pairs.\"\"\"\n",
    "    \n",
    "    event_collection_u = set()\n",
    "    for event in event_collection:\n",
    "        # transform the mapping (untangible) 'event' into the collection (tangible) 'event'\n",
    "        event2 = such_that2(event, space)\n",
    "        event_collection_u.update(set(event2.keys()))\n",
    "    return sum([space[o] for o in event_collection_u])\n",
    "    \n",
    "def p_and(event_collection, space): \n",
    "    \"\"\"The probability of an event collection intersection, given a sample space of equiprobable outcomes. \n",
    "    event: a collection of predicates that are true of outcomes in the event. \n",
    "    space: a set of outcomes or a probability distribution of {outcome: frequency} pairs.\"\"\"\n",
    "   \n",
    "    lst_of_sets = []\n",
    "    for e in event_collection:\n",
    "        lst_of_sets.append({o for o in list(space.keys()) if e(o)})\n",
    "    # Call the intersection() method on the first set object.\n",
    "    # Pass all sets as arguments into the intersection() method by unpacking the list with the asterisk operator *list.\n",
    "    #print(lst_of_sets[0].intersection(*lst_of_sets))\n",
    "    return sum([space[o] for o in lst_of_sets[0].intersection(*lst_of_sets)])\n",
    "\n",
    "is_predicate = callable\n",
    "\n",
    "def such_that2(predicate, space): \n",
    "    \"\"\"The outcomes in the sample pace for which the predicate is true.\n",
    "    Space is a ProbDist, so return a ProbDist {outcome: frequency,...} with outcomes where predicate(element) is true.\"\"\"\n",
    "    return ProbDist({o:space[o] for o in space if predicate(o)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24882071317004043"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_and({second_girl, first_boy}, DK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see this is exactly the value for the `'BG'` key of our sample space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24882071317004043"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DK['BG']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7517332091085962"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_or({second_girl, first_boy}, DK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7517332091085962"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DK['BG'] + DK['GG'] + DK['BB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now answer the **question 2** as to whether the sex of the second child is *more likely* or *less likely* to be the same as the first child, by evaluating first:\n",
    "\n",
    "- The probability of a second girl *given that the first child was a girl* (a [conditional probability](https://en.wikipedia.org/wiki/Conditional_probability))\n",
    "- The probability of a second girl *given that the first child was a boy* (a conditional probability)\n",
    "- The probability of a second boy *given that the first child was a boy* (a conditional probability)\n",
    "- The probability of a second boy *given that the first child was a girl* (a conditional probability)\n",
    "\n",
    "The average of the ***first*** two probabilities above represents the probability of a second girl (*irrespective* of any other conditions)! It's a [marginal probability](https://en.wikipedia.org/wiki/Marginal_distribution) in our problem.\n",
    "\n",
    "The average of the ***last*** two probabilities above represents the probability of a second boy, a **marginal probability** in our problem.\n",
    "\n",
    "These are the first two conditional probabilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(A | B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48471942072973107"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p(second_girl and first_boy, DK) <-- does not work, so do this instead:\n",
    "p(second_girl, such_that(first_boy, DK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4898669165584115, 0.48471942072973107)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(second_girl, such_that(first_girl, DK)), p(second_girl, such_that(first_boy, DK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the last two conditional probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5101330834415885, 0.5152805792702689)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(second_boy, such_that(first_girl, DK)), p(second_boy, such_that(first_boy, DK))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above says that the sex of the second child is more likely to be the same as the first child, by about 1/2 a percentage point.\n",
    "\n",
    "And so, the **marginal probabilities** we are after are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4872931686440713"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_second_girl = (p(second_girl, such_that(first_girl, DK)) + p(second_girl, such_that(first_boy, DK))) /2\n",
    "p_second_girl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5127068313559286"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_second_boy = (p(second_boy, such_that(first_girl, DK)) + p(second_boy, such_that(first_boy, DK))) / 2\n",
    "p_second_boy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In conclusion we see that the probability of a girl is somewhere between 48% and 49%, but slightly different between the first or second child, that the sex of the second child is more likely to be the ***same*** as the first child, by about 1/2 a percentage point, and that Danes are more likely to have a ***boy*** as the younger child, rather than a girl, by 2% points. \n",
    "\n",
    "Cooool..! We got all that through *programming*. No math! By spelling out our sets and writing down our logic.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"ipynb.images/pretty-bunny.gif\" />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 6. M&Ms and an introduction to Bayes' Theorem\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/9/97/M%26M_spokescandies.jpeg\" />\n",
    "</center>\n",
    "\n",
    "[Bayes's theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) describes the probability of an event $A$, based on prior knowledge of conditions $E$ that might be related to the event:\n",
    "\n",
    "$$P(A \\;|\\; E) = \\frac{P(E \\;|\\; A) \\;*\\; P(A)}{ P(E) }$$\n",
    "\n",
    "Just like the `1 minus` trick for estimating probabilities, you use Bayes' theorem when $P(E \\;|\\; A)$ is easier to compute than $P(A \\;|\\; E)$.\n",
    "\n",
    "For example, if the risk of developing deadly covid19 problems is known to increase with age and medical preconditions. Bayes's theorem allows the risk to an individual of a known age or having a medical precondition to be assessed more accurately than simply assuming that the individual is typical of the population as a whole.\n",
    "\n",
    "Let's study Bayes with another experiment.\n",
    "\n",
    "Here's another urn problem (or \"bag\" problem) [from](http://allendowney.blogspot.com/2011/10/my-favorite-bayess-theorem-problems.html) prolific Python/Probability author [Allen Downey ](http://allendowney.blogspot.com/), which also happens to be a classic interview question:\n",
    "\n",
    "> The blue M&M was introduced in 1995.  Before then, the color mix in a bag of plain M&Ms was (30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan).  Afterward it was (24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown). \n",
    "A friend of mine has two bags of M&Ms, and he picks one M&M from one bag and one from the other, and he tells me that one is from 1994 and one from 1996.  He won't tell me which is which, but he gives me one M&M from each bag.  One is yellow and one is green.  What is the probability that the yellow M&M came from the 1994 bag? Well, the old M&M bags' yellow count was higher, so it must be higher, right? But how to count?\n",
    "\n",
    "To solve this problem, we'll first represent probability distributions for each bag: `bag94` and `bag96`, by using `ProbDist` and passing in dictionaries for each year:\n",
    "```python\n",
    "bag94 = ProbDist(brown=30, yellow=20, red=20, green=10, orange=10, tan=10)\n",
    "bag96 = ProbDist(...)  #fill this in, please\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "bag94 = ...\n",
    "bag96 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, define `MM` as the *joint* distribution 94-96&mdash;the sample space for picking *one* M&M from *each* bag. The outcome `'yellow green'` means that a yellow M&M was selected from the 1994 bag and a green one from the 1996 bag. We will use a *set comprehension*.\n",
    "\n",
    "Uhhhh... What do we use for sets again? Is it `[`, or `(`, or `{`?\n",
    "\n",
    "To note:\n",
    "* We are using a python *set* because we care about dictionaries, and dictionary keys are unique\n",
    "* You can also think in terms of JSON objects\n",
    "\n",
    "```python\n",
    "def joint(A, B, sep=''):\n",
    "    \"\"\"The joint distribution of two independent probability distributions. \n",
    "    Result is all entries of the form {a+sep+b: P(a)*P(b)}\"\"\"\n",
    "    return ProbDist({a + sep + b: A[a] * B[b]\n",
    "                    for ...\n",
    "                    for ...})\n",
    "\n",
    "MM = joint(bag94, bag96, ' ')\n",
    "MM\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def joint(A, B, sep=''):\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's look at the \"One is yellow and one is green\" part:\n",
    "\n",
    "```python\n",
    "def yellow_and_green(outcome): return 'yellow' in outcome and 'green' in outcome\n",
    "\n",
    "such_that(...) # fill this in\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we can answer the question: given that we got a yellow and a green (but don't know which comes from which bag), what is the probability that the yellow came from the 1994 bag?\n",
    "\n",
    "```python\n",
    "def yellow94(outcome): return ...\n",
    "\n",
    "p(yellow94, such_that(...)) # fill this in\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "So there is a 74% chance that the yellow comes from the 1994 bag. We were *right* about our hunch :-)\n",
    "\n",
    "Answering this question was straightforward: just like all the other probability problems, we simply create a sample space, and use `p` to pick out the probability of the event in question, given what we know about the outcome. This is the 'mechanistic' way of obtaining our answer.\n",
    "\n",
    "# With Math\n",
    "We can *also* solve it using *Bayes' Theorem*, and this is as good as any's introduction to naive Bayes theory: We are asked about the probability of an event (M&M94 --> M&M96) given the evidence (M&M94 is yellow, M&M96 green), which is not immediately available. However the probability of the evidence, given the event is readily available!  \n",
    "\n",
    "Before we see the colors of the M&Ms, there are two hypotheses, `A` and `B`, both with equal probability:\n",
    "\n",
    "    A: first M&M from 94 bag, second from 96 bag\n",
    "    B: first M&M from 96 bag, second from 94 bag\n",
    "    P(A) = P(B) = 0.5\n",
    "    \n",
    "Then we get some evidence:\n",
    "    \n",
    "    E: first M&M yellow, second green\n",
    "    \n",
    "We want to know the probability of hypothesis `A`, given the evidence:\n",
    "    \n",
    "    P(A | E)\n",
    "    \n",
    "That's not easy to calculate (except by enumerating the sample space, which is what we did above). But Bayes Theorem says:\n",
    "    \n",
    "    P(A | E) = P(E | A) * P(A) / P(E)\n",
    "    \n",
    "The quantities on the *right-hand-side* are easier to calculate:\n",
    "    \n",
    "    P(E | A) = 20/100 * 20/100 = 0.04\n",
    "    P(E | B) = 10/100 * 14/100 = 0.014\n",
    "    P(A)     = 0.5\n",
    "    P(B)     = 0.5\n",
    "    P(E)     = P(E | A) * P(A) + P(E | B) * P(B) \n",
    "             = 0.04     * 0.5  + 0.014    * 0.5   =   0.027\n",
    "             \n",
    "Where did the probability of the evidence P(E) formula come from?\n",
    "\n",
    "There are two possibilities of getting the evidence: A and B, a *union* and so we sum their probabilities. The joint probability of the evidence *and* case A is a succession or *intersection*, so it must be a product of their probabilities: P(E|A).P(A). Likewise for the case B: P(E|B).P(B) \n",
    "    \n",
    "And so we can get a final answer:\n",
    "    \n",
    "    P(A | E) = P(E | A) * P(A) / P(E) \n",
    "             = 0.04     * 0.5  / 0.027 \n",
    "             = 0.7407407407\n",
    "             \n",
    "Bayes Theorem allows you to do less calculation at the cost of more algebra; that is a great trade-off if you are working with pencil and paper (like in interview situations). Enumerating the state space allows you to do less algebra at the cost of more calculation; often a good trade-off if you have a computer. But regardless of the approach you use, it is important to understand Bayes theorem and how it works.\n",
    "\n",
    "Bayes' theorem will be our introduction to more advanced statistics, which we'll cover *next* week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data? Big Deal..\n",
    "\n",
    "So far, we have talked of an *outcome* as being a single state of the world. But it can be useful to break that state of the world down into *many* components. We call these components **random variables**. For example, when we consider an experiment in which we roll *two* dice and observe their sum, we could model the situation with *two* random variables, one for each die (our representation of outcomes has been doing that implicitly all along, when we concatenate two parts of a string, but the concept of a random variable makes it official.)\n",
    "\n",
    "Remember [this](https://www.mathsisfun.com/data/quincunx.html) experiment?\n",
    "\n",
    "The **Central Limit Theorem** states that if you have a collection of random variables and sum them up, then the *larger* the collection, the *closer* the sum will be to a *normal distribution* (also called a *Gaussian distribution* or a *bell-shaped curve*). This illustrates why Data Science with **Big Data** is not really a challenge at all (other than how to store and compute with data that is larger than the RAM on your laptop)! If you have tons of data, then frequentist and Bayesian statistics coincide, there is little doubt about outcomes, and it is clear which outcome to place your bets on.\n",
    "\n",
    "The challenge is with **Small Data**, where there is doubt and you have no idea of the pdf, and you have to painstakingly  build a model with parameters, and refine your parameters so the model fits the data.\n",
    "\n",
    "Why do accidents happen in autonomous vehicles when it snows, at dusk, when the moon is behind a traffic light, and a pedestrian is crossing the street on a red light? Precisely because we don't have Big Data about that use case, and the autonomous car has **no idea what to do**! Uber needs *you* to program it for the Small DAta cases. Big Data is piece of cake!\n",
    "\n",
    "This is a good time to introduce the concept of a **histogram**, which is essentially a probability density function that we get from data collection or from analytic curves. If we have the 2D function `f(x,y)`, we can estimate the discretized probability density function (pdf) of `f` by computing how many values of `f` fall into specific bins with prespecified values of y.\n",
    "\n",
    "Plot the pdf of the following graph:\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/graph.png\" width=200 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Celtics\n",
    "\n",
    "As another example, let's take 5 random variables reprsenting the per-game scores of the 5 Celtics starters for the 2018 season (Jayson Tatum or JT, Jaylen Brown or JB, Terry Rozier or TR, Al Horford or AL, and Aron Baynes or AB), and then sum them together to form the team score. The scores here are imaginary to introduce statistical function `gauss`, `triangular`, `vonmisesvariate`, and `uniform`. For the real season scores of this exciting and ultimately deflating season, visit [here](http://www.nba.com/celtics/stats?sort=PTS). Each random variable/player is represented as a probability distribution function; calling the function returns a single sample from the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import gauss, triangular, choice, vonmisesvariate, uniform\n",
    "\n",
    "def JT(): return posint(gauss(15.1, 3) + 3 * triangular(1, 4, 13)) # 30.1\n",
    "def JB(): return posint(gauss(10.2, 3) + 3 * triangular(1, 3.5, 9)) # 22.1\n",
    "def TR(): return posint(vonmisesvariate(30, 2) * 3.08) # 14.0\n",
    "def AH(): return posint(gauss(6.7, 1.5) if choice((True, False)) else gauss(16.7, 2.5)) # 11.7\n",
    "def AB(): return posint(triangular(5, 17, 25) + uniform(0, 30) + gauss(6, 3)) # 37.0\n",
    "\n",
    "def posint(x): \"Positive integer\"; return max(0, int(round(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is a function to sample a random variable *k* times, show a histogram of the results, and return the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def repeated_hist(rv, bins=10, k=100000):\n",
    "    \"Repeat rv() k times and make a histogram of the results.\"\n",
    "    samples = [rv() for _ in range(k)]\n",
    "    plt.hist(samples, bins=bins, color = 'g')\n",
    "    return mean(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two top-scoring players, Jayson Tatum (JT) and Jaylen Brown (JB), have scoring distributions that are slightly skewed from normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_hist(JT, bins=range(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_hist(JB, bins=range(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two players have bi-modal distributions; some games they score a lot, some games not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_hist(TR, bins=range(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_hist(AH, bins=range(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fifth \"player\" (actually the sum of all the other players on the team, not really Aron Baymes, or they would have easily defeated the Cavaliers!) looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_hist(AB, bins=range(60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead, add more players to the Celtics, each with their own scoring distribution. Modify and use the below template:\n",
    "```python\n",
    "def XX(): return posint(gauss(15.1, 3) + 3 * triangular(1, 4, 13)) # 30.1\n",
    "```\n",
    "\n",
    "Add then in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the team score to be the sum of the five players, and look at the distribution. Don't forge to add your own players!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GSW(): return JT() + JB() + TR() + AH() + AB()  # + ... add your own players!\n",
    "\n",
    "repeated_hist(GSW, bins=range(70, 160, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, this looks very much like a **normal distribution**! \n",
    "\n",
    "The Central Limit Theorem holds, and that is why the statistics of NBA teams are so *predictable*. \n",
    "\n",
    "When you have a lot of data, you have little doubt, and the Data Science is *easy*. Deep Learning ANNs are very good at building hypersurfaces that match all the state variables of past outcomes. When you are asked to predict new outcomes based on a partial subset of state variables (independent variables), machines just project the lower dimensional hypersurface onto the higher dimensional hypersurface (stored in very compact form) and fill in the missing dimensions in order to predict the dependent variables. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/manifold.jpg\" width=300 />\n",
    "</center>\n",
    "\n",
    "\n",
    "We saw just a few days ago how machines can even predict non-linear functions if we give them tons of data, but when the data is not enough, they *cannot* predict!\n",
    "\n",
    "So what do we do when we *don't have enough data*? Do we just give up? We'll see next week how **Bayesian statistics** come to the rescue, *when we have to deal with doubt*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion¶\n",
    "We've built a framework for estimating probabilities that will come in handy when you are asked to build data models. We *started playing* with **Bayes' theorem**, a *pillar* of data science, and we started learning about typical Data Science interview questions, which you should *always* answer in python because you *will* run out of space on the whiteboard if you use squiggly brackets and other languages! \n",
    "\n",
    "Next week we move on to statistical modeling and inference. \n",
    "\n",
    "**Modeling** (one `l` or 2 `l`s? See [here](https://www.grammarly.com/blog/modeling-or-modelling/)) happens when data is scarce and precious and hard to obtain, for example in social sciences and settings where it is difficult to conduct large-scale controlled experiments. With small data it is important to quantify uncertainty and that’s precisely what Bayesian approaches are good at. **Inference** refers to how you learn parameters of your model (Markov Chain Monte Carlo, or MCMC, albeit computationally expensive, is one of the most important methods for statistical inference), which is especially important with Bayesian Machine Learning, where we can actually inquire with Machines why this or that action was undertaken.\n",
    "\n",
    "*Alexa, why did you lower the temperature in the bedroom?* **Because your wife told me that whenever you start snoring, John, colder tempreatures make you bundle up under the cover and you snore less**. \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/echo.jpg\" width=200 />\n",
    "</center>\n",
    "\n",
    "Best advice for interviews: Be explicit about what the problem says, have the interviewer verify the working hypotheses, be methodical about defining the sample space, be careful in counting the number of outcomes in the numerator and denominator, and finally use Bayes' theorem (or 1 minus the negation) whenever possible because you will be doing calculations by hand on a whiteboard!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/jobinterview.jpg\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework for next week: Introduction to Data Science for Sports\n",
    "\n",
    "You will exercise your knowledge of probabilities and write down logic with python.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/F1GP21.png\" width=900 />\n",
    "</center>\n",
    "\n",
    "**Question 1** (20 points) Here are the driver [standings](https://www.formula1.com/en/drivers.html) for the year 2021. What a year it was! Mercedes' Lewis Hamilton *lost* to Red Bull Racing's Max Verstappen!. Here are the last 3 F1 races for the season, that happened while we were learning data science! \n",
    "- Italy GP: Date: September 10\n",
    "- Russia GP: Date: September 24\n",
    "- Turkey GP: Date: October 10\n",
    "\n",
    "Suppose we are just starting the semester, right before the Italy Grand Prix (this coming weekend) and the Russia Grand Prix the weekend after, as you can see [schedule](https://www.formula1.com/en/racing/2021.html). \n",
    "\n",
    "The 2021 driver standings are already [tabulated](https://www.formula1.com/en/drivers.html) and Max beat Lewis! But let's ***assume*** these standings ***before the Italy GP***. \n",
    "\n",
    "Given these standings (please do not use team standings given on the same Web site, use ***driver standings***), what is the Probability Distribution for each F1 driver to win the Italy Grand Prix? \n",
    "\n",
    "What is the Probability Distribution for each F1 driver to win ***both*** the Italy and the Russia Grand Prix? \n",
    "\n",
    "What is the probability for Red Bull Racing to win ***both*** races? \n",
    "\n",
    "What is the probability for Red Bull Racing to win ***at least*** one race? \n",
    "\n",
    "What is the probability for Red Bull Racing to win ***all three*** races? \n",
    "\n",
    "Note that Red Bull Racing, and each other racing team, has ***two*** drivers per race. Use individual driver standings (not team standings), and use the same *final* [standings](https://www.formula1.com/en/results.html/2020/drivers.html) for both races.\n",
    "\n",
    "**Question 2** (30 points) If Red Bull Racing wins the first race, what is the probability that Red Bull Racing wins the next one? If Red Bull Racing wins at least one of these two races, what is the probability Red Bull Racing wins both races? How about Mercedes, McLaren, and Ferrari?\n",
    "\n",
    "**Question 3** (50 points) Red Bull Racing wins at least one of these two races on a ***rainy day***. What is the probability Red Bull Racing wins ***both*** races, assuming races can be held on either rainy, sunny, cloudy, snowy or foggy days? Also assume that rain, sun, clouds, snow, and fog are the *only possible weather conditions* on race tracks, and that they're *equiprobable*.\n",
    "\n",
    "You need to provide *proof* for your answers. `I think it's one in a million because Red Bull Racing ***sucks*** and I like Mercedes ***a lot more***` is not a good answer. Leverage the counting framework in this workbook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use ItalyGrandPrix, or `IGP` to denote the Probability Distribution given by F1 driver wins. Write driver initials as keys and driver wins as values in a dictionary that you pass to our function `ProbDist`.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IGP = ProbDist(\n",
    "    MV = 262,\n",
    "    LH = 256,\n",
    "    VB = 177)\n",
    "IGP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework hint\n",
    "\n",
    "**Conditional probabilities** involve ***prior*** conditions and ***new*** predicates, which is not the same as applying *one* condition on the *entire universe of outcomes*!\n",
    "\n",
    "A conditional probability is usually expressed as A | B (present/future condition A given past condition B). \n",
    "\n",
    "> Example: picking a yellow M&M from the '94 bag given that i've already picked yellow and green M&Ms from each bag. \n",
    "\n",
    "A conditional probability involves a **past event**, and a **present or future condition** (or predicate). So, computing a conditional probability where you specify *all* (past and present/future) conditions (or predicates) in the 1st argument of our p function is *not* possible: You need to first filter your entire universe of outcomes using the 2nd argument of our p function (to specify the past event by limiting the universe of all possible outcomes), and *then* specify the (future, or filter) condition using the 1st argument of our p function.\n",
    "\n",
    "# Reading assignment\n",
    "\n",
    "Please read Chapter 6 of *Mathematics for Machine Learning*, Deisenroth et al."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
